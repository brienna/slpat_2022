{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code walkthrough\n",
    "\n",
    "In this notebook, I give a walkthrough of the code that I wrote for analysis. First, I demonstrate how you can use BERT to predict a word using the Masked Language Model (MLM) approach. This is to get a feel for how the model works. I also address a limitation of transformers' Pipeline, which to the need to write more code from \"scratch\" (though still using transformers).\n",
    "\n",
    "### Table of Contents:\n",
    "- [1. BERT demo](#1-bert-demo)\n",
    "- [2. A limitation of transformers' Pipelines](#2-a-limitation-of-transformers-pipelines)\n",
    "- [3. Code walkthrough](#3-code-walkthrough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BERT demo\n",
    "\n",
    "I chose `bert-base-uncased` ([source](https://huggingface.co/models?sort=downloads&search=BERT)), since it was the most-downloaded version of BERT available on HuggingFace. This is similar to the reasoning by Kirk et al. (2021), where they discuss in their appendix their reasons for taking a most-downloaded approach with GPT-2. \n",
    "\n",
    "The output is in the form of logits, which are the raw scores from the last layer. These logits are not probabilities until they go through a softmax function. The output is structured as a 3D tensor, where each nested 2D array contains logits for each token in a sentence, mapped against every single word in BERT's vocabulary. After applying a softmax function to these logits, you obtain a probability distribution over the vocabulary for each token in the sentence. \n",
    "\n",
    "The internal `encode_plus` function performs the following steps:\n",
    "1. Tokenize the input sentence\n",
    "2. Add the `[CLS]` (Classification) and `[SEP]` (Separator) tokens\n",
    "3. Encode the tokens into their corresponding IDs\n",
    "4. Pad or truncate the sentence to the maximum length allowed\n",
    "5. Create attention masks which explicitly differentiate real tokens from `[PAD]` tokens\n",
    "6. Return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(masked_sentence, k=10, verbose=False):\n",
    "    candidates = []\n",
    "    input = tokenizer.encode_plus(masked_sentence, return_tensors = \"pt\")\n",
    "    mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "    output = model(**input)\n",
    "    softmax = F.softmax(output.logits, dim = -1)\n",
    "    mask_word = softmax[0, mask_index, :]\n",
    "    top_k = torch.topk(mask_word, k, dim = 1)[1][0]\n",
    "    for token in top_k:\n",
    "        word = tokenizer.decode([token])\n",
    "        new_sentence = masked_sentence.replace(tokenizer.mask_token, word)\n",
    "        if verbose:\n",
    "            print(new_sentence)\n",
    "        candidates.append(word)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plans',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'something',\n",
       " 'dinner',\n",
       " 'planned',\n",
       " 'pancakes',\n",
       " 'it',\n",
       " 'company',\n",
       " 'one']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the top 10 candidates for filling in the masked word\n",
    "text = \"I had \" + tokenizer.mask_token + \" for dinner.\"\n",
    "predict_words(text, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ';', '?', '|', '!', ':', '...', '-', ',', '[UNK]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to include a period to end the sentence. See what happens when we don't\n",
    "text = \"A deaf person is \" + tokenizer.mask_token \n",
    "predict_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " 'allowed',\n",
       " 'blind',\n",
       " 'born',\n",
       " 'acceptable',\n",
       " 'possible',\n",
       " 'preferred',\n",
       " 'impossible',\n",
       " 'excluded',\n",
       " 'eligible']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to when we include a period\n",
    "text = \"A deaf person is \" + tokenizer.mask_token + \".\"\n",
    "predict_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A limitation of transformers' Pipelines\n",
    "\n",
    "In the `transformers` library, Pipelines are a great and easy way to use models for inference, as they abstract away most of the complex code and offer a simple API dedicated to several tasks, including [Masked Language Modeling](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.FillMaskPipeline). To perform this task with the pipeline, you can simply do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ffcec61976443c911e5f2b0112e6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.021829674020409584,\n",
       "  'token': 12419,\n",
       "  'token_str': 'deaf',\n",
       "  'sequence': 'a deaf person is eligible.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_masker = pipeline(model=\"bert-base-uncased\")\n",
    "fill_masker(inputs=\"A [MASK] person is eligible.\",\n",
    "            targets=[\"deaf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the result comes as a list of dictionaries with the following keys: \n",
    "\n",
    "- sequence (str) — The corresponding input with the mask token prediction.\n",
    "- score (float) — The corresponding probability.\n",
    "- token (int) — The predicted token id (to replace the masked one).\n",
    "- token_str (str) — The predicted token (to replace the masked one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the pipeline only supports inputs with exactly one token masked. As explained in [this]([comment](https://github.com/huggingface/transformers/issues/17374#issuecomment-1134388133)) GitHub comment, the pipeline works at the token level, not the word level, so you can't use targets which are multi-token. There is [some work](https://github.com/huggingface/transformers/pull/10222) underway to add this functionality, but it's still in-progress and experimental. \n",
    "\n",
    "Unfortunately, this means that I can't take the easy route here, because one of my targets, \"abled,\" is a multi-token word, because it doesn't exist in BERT's dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The specified target token `abled` does not exist in the model vocabulary. Replacing with `able`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.00032161909621208906,\n",
       "  'token': 2583,\n",
       "  'token_str': 'able',\n",
       "  'sequence': \"i'm an able person.\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker(inputs=f\"I'm an [MASK] person.\", targets=['abled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the multi-token nature of \"abled\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids: [2583, 2094]\n",
      "Tokens: ['a b l e', '# # d']\n",
      "Decoded word: abled\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode('abled', add_special_tokens=False)\n",
    "print('Token ids:', token_ids)\n",
    "\n",
    "tokens = [tokenizer.decode(i) for i in token_ids]\n",
    "print('Tokens:', tokens)\n",
    "\n",
    "decoded_word = tokenizer.decode(token_ids)\n",
    "print('Decoded word:', decoded_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code walkthrough\n",
    "\n",
    "Since I cannot use a transformers pipeline, I'm going to have to write the code myself. \n",
    "\n",
    "### 3.1. Predicting the association of the target to the attribute\n",
    "\n",
    "Calculate increased log probability score, testing on one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'Sentence_TM': 'A person who is [MASK] is aberrant.', \n",
    "     'Sentence_AM': 'A person who is abled is [MASK].', \n",
    "     'Sentence_TAM': 'A person who is [MASK] is [MASK].', \n",
    "     'Target': 'abled',\n",
    "     'Attribute': 'aberrant'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute target probability - the probability that \"deaf\" would be predicted in the sentence \"A person who is [MASK] is aberrant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1037,  2711,  2040,  2003,   103,  2003, 14863, 28327,  2102,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map each token to its id in BERT's vocabulary\n",
    "input = tokenizer.encode_plus(x['Sentence_TM'], return_tensors='pt')\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note position of the masked target\n",
    "mask_index = torch.where(input['input_ids'][0] == tokenizer.mask_token_id)[0] # [0] assumes that the target is the first mask\n",
    "mask_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2583, 2094]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get ids for target, which helps account OOV (out of vocab) targets by taking the ids of sub-tokens\n",
    "target_ids = tokenizer.encode(x['Target'], add_special_tokens=False)\n",
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass sentence to BERT for predictions\n",
    "with torch.no_grad():\n",
    "    output = model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.6631,  -6.6060,  -6.6229,  ...,  -5.9789,  -5.7954,  -3.9998],\n",
       "        [-15.5440, -15.6921, -15.7234,  ..., -14.6199, -12.0014,  -8.5450],\n",
       "        [-11.2371, -11.7828, -11.5929,  ...,  -9.5335,  -8.3642,  -7.5928],\n",
       "        ...,\n",
       "        [ -7.9006,  -8.1307,  -8.1574,  ...,  -7.5839,  -7.0528,  -2.8307],\n",
       "        [-10.5031, -10.3202, -10.7235,  ...,  -8.9248,  -9.5005,  -4.0732],\n",
       "        [-11.9663, -11.9535, -12.1448,  ..., -10.3659, -10.2758,  -8.3879]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the outputs from the final layer of BERT as embeddings\n",
    "last_hidden_state = output.logits[0]\n",
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9618e-07, 5.2531e-07, 5.1650e-07,  ..., 9.8352e-07, 1.1815e-06,\n",
       "         7.1169e-06],\n",
       "        [4.3155e-18, 3.7217e-18, 3.6068e-18,  ..., 1.0874e-17, 1.4914e-16,\n",
       "         4.7278e-15],\n",
       "        [2.6553e-13, 1.5386e-13, 1.8603e-13,  ..., 1.4587e-12, 4.6966e-12,\n",
       "         1.0158e-11],\n",
       "        ...,\n",
       "        [1.6209e-13, 1.2877e-13, 1.2537e-13,  ..., 2.2249e-13, 3.7841e-13,\n",
       "         2.5797e-11],\n",
       "        [1.9471e-15, 2.3378e-15, 1.5619e-15,  ..., 9.4363e-15, 5.3065e-15,\n",
       "         1.2074e-12],\n",
       "        [2.6752e-14, 2.7097e-14, 2.2379e-14,  ..., 1.3256e-13, 1.4505e-13,\n",
       "         9.5814e-13]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the embedding so that the sum of all token probabilities is 1\n",
    "softmax = F.softmax(last_hidden_state, dim=-1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the target's probability, given the context provided by the attribute.\n",
    "\n",
    "This is the probability or raw association score for the sentence where only the target is masked, e.g. `\"A [MASK] person is aberrant.\"` We are asking BERT to predict the probability that `[MASK]` is, in fact, the target (e.g., `abled` or `disabled`). In other words, how probable is it that the masked sentence could be `\"A abled person is aberrant.\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2687156e-08"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_target = np.prod([softmax[mask_index, t].numpy()[0] for t in target_ids])\n",
    "prob_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the target's prior probability, given no context. \n",
    "\n",
    "When measuring bias in BERT, comparing raw association scores (`prob_target`) can be misleading because the likelihood of a token is influenced by all other tokens in the sentence. Each token's probability depends on its surrounding context. Therefore, the likelihood of a target can change depending on the presence or absence of an attribute. Additionally, the likelihoods of different targets can be influenced differently by the same attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.42944e-08"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the index to use to identify the target mask, which helps when there are multiple masks\n",
    "# note that this index depends on the sentence template, and with our templates the target is always the first mask\n",
    "target_idx = 0 \n",
    "\n",
    "input = tokenizer.encode_plus(x['Sentence_TAM'], return_tensors='pt')\n",
    "mask_index = torch.where(input['input_ids'][0] == tokenizer.mask_token_id)[target_idx]\n",
    "target_ids = tokenizer.encode(x['Target'], add_special_tokens=False)\n",
    "with torch.no_grad():\n",
    "    output = model(**input)\n",
    "last_hidden_state = output.logits[0]\n",
    "softmax = F.softmax(last_hidden_state, dim=-1)\n",
    "prob_prior = np.prod([softmax[mask_index, t].numpy()[0] for t in target_ids])\n",
    "prob_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the adjusted association score that is corrected by the prior probability. \n",
    "\n",
    "For interpretation:\n",
    "\n",
    "- A negative adjusted association score means the target's probability is lower than the prior probability, indicating the attribute's context decreased the likelihood of predicting the target.\n",
    "- A positive adjusted association score means the target's probability is higher than the prior probability, indicating the attribute's context increased the likelihood of predicting the target.\n",
    "\n",
    "In other words:\n",
    "\n",
    "- If the association score is negative, the attribute’s context decreased the probability that BERT predicts the target.\n",
    "- If the association score is positive, the context increased the target’s probability of being predicted.\n",
    "\n",
    "Therefore, if BERT has a higher positive association score for one target over another in a given context, it indicates that BERT is more likely to associate that target with the stereotype context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7741258"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assoc_score = np.log(prob_target/prob_prior)\n",
    "assoc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sentence \"A person who is abled is aberrant,\" where the target is \"abled\" and the attribute is \"aberrant,\" the association score between the target and the attribute is 0.77. Since this score is above 0, it suggests that BERT positively associates the target with the attribute. In other words, BERT is more likely to predict the \"abled\" target given the presence of the attribute \"aberrant\" in the sentence, compared to the sentence where the attribute is masked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This uses np.prod for calculating the probability... This is something that I added, because of multi-token targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slpat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
